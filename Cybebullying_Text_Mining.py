# -*- coding: utf-8 -*-
"""CA2_TextMining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Khzh6Qqvb5Gz5Kh_ngQn-ntj1vzAO1OM
"""

# imblearn library installed using pip install imblearn
!pip install imblearn

# importing necessary libraries
import re, nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
import joblib

# Importing dataset and reading it as dataframe
df = pd.read_csv("/content/drive/MyDrive/Datasets/cyberbullying_tweets.csv")
pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells
pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window

# Mapping the values to positive and negative labels
df['cyberbullying_type'] = df['cyberbullying_type'].map({'age':1, 'ethnicity':1,'gender':1,'religion':1,'other_cyberbullying':1, 'not_cyberbullying':0})

# Viewing the dataset
df.shape
df.info()

# Cleaning tweet_text
def cleaner(tweet_text):
# Removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'
    soup = BeautifulSoup(tweet_text, 'lxml')
    souped = soup.get_text()
    re1 = re.sub(r"(@|http://|https://|www|\\x)\S*", " ", souped) # Substituting @mentions, urls, etc with whitespace
    re2 = re.sub("[^A-Za-z]+"," ", re1) # Substituting any non-alphabetic character that repeats one or more times with whitespace

    tokens = nltk.word_tokenize(re2) # Tokenizing the text using nltk word_tokenize function
    lower_case = [t.lower() for t in tokens] # Converting to lowercase

    stop_words = set(stopwords.words('english')) # Removing the stop words
    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))

    wordnet_lemmatizer = WordNetLemmatizer()
    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result] # Lemmatizing
    return lemmas

# Applying the cleaner function on the tweet text
df['cleaned_tweet_text'] = df.tweet_text.apply(cleaner)
df = df[df['cleaned_tweet_text'].map(len) > 0] # Removing rows where the cleaned tweet text has a length of 0
print("Printing top 5 rows of dataframe showing original and cleaned reviews....")
print(df[['tweet_text','cleaned_tweet_text']].head())
# Joining tokens to create strings as TfidfVectorizer does not accept tokens as input
df['cleaned_tweet_text'] = [" ".join(row) for row in df['cleaned_tweet_text'].values]
data = df['cleaned_tweet_text']

# Converting the unstructured 'tweet_text' column to a TF-IDF matrix
Y = df['cyberbullying_type'] # target column
# min_df=.00063 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents
# for it to be considered as a variable (47692 * .00063 = 30)
tfidf = TfidfVectorizer(min_df=.00063, ngram_range=(1,3))

tfidf.fit(data) # Fitting the TF-IDF vectorizer to the data to learn the vocabulary
df_tfidf = tfidf.transform(data) # Transforming the data to create TF-IDF values
pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary_cyberbullying.csv', header=False, index=False) # Saving the vocabulary to a CSV file
print("Shape of tfidf matrix: ", df_tfidf.shape)

# Implementing Support Vector Classifier with a linear kernel
model1 = LinearSVC() # Initializing LinearSVC model with default parameters (kernel = 'linear' and C = 1)

# Running cross-validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation
scores=[]
iteration = 0
smote = SMOTE(random_state = 101)
for train_index, test_index in kf.split(df_tfidf,Y):
    iteration += 1
    print("Iteration ", iteration)
    X_train, Y_train = df_tfidf[train_index], Y.iloc[train_index]
    X_test, Y_test = df_tfidf[test_index], Y.iloc[test_index]
    X_train,Y_train = smote.fit_resample(X_train,Y_train) # Balancing training data
    model1.fit(X_train, Y_train) # Fitting SVC
    Y_pred = model1.predict(X_test)
    score = metrics.recall_score(Y_test, Y_pred) # Calculating recall
    print("Cross-validation recall: ", score)
    scores.append(score) # Appending cross-validation recall for each iteration
mean_recall = np.mean(scores)
print("Mean cross-validation recall: ", mean_recall)

# Implementing Naive Bayes Classifier
model2 = MultinomialNB()

# Running cross-validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation
scores=[]
iteration = 0
smote = SMOTE(random_state = 101)
for train_index, test_index in kf.split(df_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)
    X_train, Y_train = df_tfidf[train_index], Y.iloc[train_index]
    X_test, Y_test = df_tfidf[test_index], Y.iloc[test_index]
    X_train,Y_train = smote.fit_resample(X_train,Y_train) # Balancing training data
    model2.fit(X_train, Y_train) # Fitting NBC
    Y_pred = model2.predict(X_test)
    score = metrics.recall_score(Y_test, Y_pred) # Calculating Recall
    print("Cross-validation recall: ", score)
    scores.append(score) # Appending cross-validation Recall for each iteration
mean_recall_score = np.mean(scores)
print("Mean cross-validation recall: ", mean_recall_score)